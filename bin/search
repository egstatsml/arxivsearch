#!/usr/bin/env python

import arxivpy
import feedparser
import datetime
import pandas as pd

from arxivsearch.category import category, get_category

import os
import argparse
import sys


def format_dataframe(df):
    #check to see if the data frame is empty
    #if it is, return an empty data frame
    if(df.empty):
        #set the title and publish date to just say
        #no entries for this week
        df = pd.DataFrame(data =
                          {'title':['---', 'No papers found this week'],
                           'publish_date':['---', 'NA'],
                           'main_author':['---', 'NA']})
        return df
    
    #otherwise the dataframe hase some entries and we will format it
    df = df.drop(['id', 'terms', 'term',
                  'authors', 'abstract', 'comment',
             'journal_ref', 'pdf_url'], axis=1)
    #remove repeated entries
    df = df.drop_duplicates(subset=['title'])
    types = df.apply(lambda x: pd.api.types.infer_dtype(x.values))
    for col in types[types=='unicode'].index:
        df[col] = df[col].apply(lambda x: x.encode('utf-8').strip())
    
    df['title'] = df.apply(
        lambda row:'[{}]({})'.format(row['title'],
                                     row['url']), axis=1)
    df['publish_date'] = df.apply(
        lambda row: row['publish_date'].strftime("%d-%m-%Y"), axis=1)
    #now drop the unnecesary columns
    df = df.drop(['url'], axis=1)
    cols = df.columns
    # Create a new DataFrame with just the markdown
    # strings
    df2 = pd.DataFrame([['---',]*len(cols)], columns=cols)
    #Create a new concatenated DataFrame
    df3 = pd.concat([df2, df])
    return(df3)



def load_prev_search(category):
    """Will load in the previous search results

    If the previous search results aren't around, will 
    return an dataframe with a single entry with the date from
    60 days ago where it will begin the search, and will remind
    you that you are awesome :)
    Args:
      category (str):
        category supplied by command line argument
    Returns:
      Dataframe with the previous search results
    """
    df_path = os.path.join('./{}/readme_df.csv'.format(category))
    if(os.path.isfile(df_path)):
        df = pd.DataFrame.from_csv(df_path, sep='|')
    else:
        start_date = (datetime.datetime.now() -
                      datetime.timedelta(days = 60)).strftime("%d-%m-%Y")
        df = pd.DataFrame(data =
                          {'title':['---', 'This is where it all Began'],
                           'publish_date':['---', start_date],
                           'main_author':['---', 'You\'re Awesome!']})
    return(df)



def most_recent_date(prev_df):
    """Will return the most recent date from the previous df"""
    # put it in a new data frame, as it allows us to find the maximum a
    # (most recent date) bit easier
    date = pd.DataFrame(
        data = {'publish_date':
                [datetime.datetime.strptime(d, '%d-%m-%Y')
                 for d in prev_df.publish_date if (d != '---')]})
    return date.publish_date.max()



def main(arguments):
    """
    main()
    Description:
    Will update the list of papers
    """
    parser = argparse.ArgumentParser(prog="main",
                                     epilog=main.__doc__)
    parser.add_argument('category', type=str, default='bnn',
                        help="category we want to search for")
    
    args = parser.parse_args(arguments)
    search_category = get_category(args.category)
    # get the previous search database
    prev_search = load_prev_search(args.category)
    valid_date = most_recent_date(prev_search)
    
    # get the search terms
    search_list = search_category.format_search()
    results = []
    for search in search_list:
        results.extend(arxivpy.query(search_query=search))

    #now wil remove any elements that are older than the specified date
    valid_date = valid_date.replace(tzinfo=None)
    valid_search = [x for x in results
                    if (x['publish_date'].date() > valid_date.date())]
    
    #convert the search results to a df
    valid_df = pd.DataFrame(valid_search)
    #format the dataframe before we convert it to markdown
    markdown_df = format_dataframe(valid_df)
    # Save as markdown
    # check the folder exists
    if(not os.path.isdir(args.category)):
        os.mkdir(args.category)
    markdown_df.to_csv(os.path.join(args.category, "new_df.csv"),
                       sep="|", index=False, encoding='utf-8')
    
if __name__ == "__main__":
    sys.exit(main(sys.argv[1:]))
